{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchmultimodal.models.flava.model import flava_model\n",
    "from torchmultimodal.transforms.flava_transform import FLAVAImageTransform\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': [152628], 'area': [85550.11445000001], 'bbox': [[39.90999984741211, 97.08000183105469, 511.2799987792969, 326.8299865722656]], 'label': [4], 'iscrowd': [False]}, {'id': [341560, 425599, 1982651], 'area': [632.9258500000003, 82680.72434999997, 12868.869650000002], 'bbox': [[265.05999755859375, 126.02999877929688, 33.86000061035156, 66.26000213623047], [20.600000381469727, 1.0700000524520874, 270.4100036621094, 382.42999267578125], [268.6099853515625, 69.66000366210938, 222.67999267578125, 88.9000015258789]], 'label': [90, 1, 81], 'iscrowd': [False, False, False]}, {'id': [435260, 474294, 517321, 562804, 565540, 1042342, 1044205, 1045269, 1045368, 1046442, 1046668, 1245662, 1370458, 1542652, 1899986, 2110381, 2110889, 2110962, 2160368, 2186237], 'area': [12333.025599999999, 11494.345149999997, 4143.89425, 15237.131249999999, 3487.6596, 1287.7461999999991, 3870.9556000000002, 612.23305, 3296.4238500000006, 546.5966499999997, 1696.7556000000002, 2122.1392499999997, 2127.915099999999, 727.9552, 21068.55, 452.47024999999974, 434.5928500000002, 977.3931, 833.0966500000003, 1088.0442999999998], 'bbox': [[172.3699951171875, 193.1699981689453, 145.38999938964844, 134.89999389648438], [416.30999755859375, 150.7100067138672, 83.69000244140625, 170.10000610351562], [113.4000015258789, 226.05999755859375, 87.29000091552734, 102.20999908447266], [0.0, 123.75, 139.8300018310547, 204.88999938964844], [281.2699890136719, 215.38999938964844, 62.66999816894531, 116.61000061035156], [250.86000061035156, 37.220001220703125, 52.11000061035156, 72.20999908447266], [136.6300048828125, 53.68000030517578, 94.29000091552734, 56.779998779296875], [225.22999572753906, 54.86000061035156, 35.459999084472656, 36.970001220703125], [94.5199966430664, 18.65999984741211, 60.150001525878906, 85.93000030517578], [332.1700134277344, 64.97000122070312, 18.170000076293945, 39.709999084472656], [299.8999938964844, 34.220001220703125, 34.90999984741211, 72.44000244140625], [95.18000030517578, 241.27000427246094, 49.4900016784668, 90.45999908447266], [133.25, 220.97999572753906, 78.63999938964844, 56.91999816894531], [203.10000610351562, 33.66999816894531, 33.720001220703125, 30.15999984741211], [104.33999633789062, 95.72000122070312, 281.989990234375, 102.62000274658203], [238.3000030517578, 81.87000274658203, 21.40999984741211, 27.790000915527344], [349.2099914550781, 66.9000015258789, 14.680000305175781, 36.4900016784668], [325.3399963378906, 25.010000228881836, 38.65999984741211, 54.11000061035156], [71.9800033569336, 206.7899932861328, 29.420000076293945, 124.81999969482422], [268.6099853515625, 32.45000076293945, 45.810001373291016, 75.52999877929688]], 'label': [1, 1, 1, 1, 1, 52, 52, 52, 52, 52, 52, 1, 8, 52, 51, 52, 52, 52, 1, 52], 'iscrowd': [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]}, {'id': [336129, 674780, 1142474, 1145089, 1158532], 'area': [4737.597749999999, 4254.8196499999995, 2210.926700000001, 4068.0025500000015, 2373.64885], 'bbox': [[467.8900146484375, 55.63999938964844, 101.25, 49.0], [364.6000061035156, 235.1199951171875, 64.02999877929688, 80.51000213623047], [225.7100067138672, 220.02999877929688, 57.5099983215332, 43.970001220703125], [34.369998931884766, 149.75999450683594, 55.650001525878906, 170.22999572753906], [110.55000305175781, 170.22999572753906, 60.459999084472656, 72.25]], 'label': [85, 47, 84, 84, 87], 'iscrowd': [False, False, False, False, False]}, {'id': [164213], 'area': [180817.83764999997], 'bbox': [[70.47000122070312, 106.43000030517578, 512.0, 431.4599914550781]], 'label': [6], 'iscrowd': [False]}, {'id': [169542, 190010, 209572, 1734846], 'area': [83685.95475, 5129.4541000000045, 2984.7365500000005, 544.6089500000005], 'bbox': [[226.52000427246094, 90.27999877929688, 404.489990234375, 326.8299865722656], [461.82000732421875, 115.31999969482422, 120.41999816894531, 108.73999786376953], [394.6199951171875, 114.62000274658203, 56.2400016784668, 74.9800033569336], [500.8299865722656, 175.36000061035156, 27.8700008392334, 42.86000061035156]], 'label': [7, 1, 1, 1], 'iscrowd': [False, False, False, False]}, {'id': [668510, 1101850, 1108728, 1176077, 1440035, 1607696, 1632673, 1648960, 1656876, 2141797, 2163198], 'area': [11799.396650000002, 98416.97530000002, 2589.2777999999985, 15808.81955, 1064.94065, 1468.044449999999, 2841.2034000000003, 1376.6005999999998, 179.68629999999973, 140.4326999999999, 15154.630249999998], 'bbox': [[533.9299926757812, 210.00999450683594, 103.55000305175781, 131.58999633789062], [185.8300018310547, 71.4800033569336, 360.80999755859375, 380.0199890136719], [85.18000030517578, 180.44000244140625, 94.11000061035156, 68.2300033569336], [449.79998779296875, 75.51000213623047, 190.1999969482422, 149.92999267578125], [321.5899963378906, 208.7100067138672, 23.850000381469727, 58.849998474121094], [409.4100036621094, 201.17999267578125, 33.40999984741211, 55.04999923706055], [42.220001220703125, 181.10000610351562, 101.63999938964844, 74.93000030517578], [157.30999755859375, 171.47999572753906, 43.61000061035156, 60.709999084472656], [242.10000610351562, 199.16000366210938, 10.470000267028809, 30.1299991607666], [232.22000122070312, 201.3800048828125, 7.630000114440918, 28.809999465942383], [222.22000122070312, 113.18000030517578, 197.22000122070312, 166.60000610351562]], 'label': [47, 73, 75, 31, 32, 64, 75, 84, 84, 84, 1], 'iscrowd': [False, False, False, False, False, False, False, False, False, False, False]}, {'id': [19655, 1153039], 'area': [166835.23820000008, 10704.127099999992], 'bbox': [[41.29999923706055, 21.43000030517578, 469.239990234375, 573.989990234375], [174.02000427246094, 533.1300048828125, 163.9600067138672, 94.91999816894531]], 'label': [64, 86], 'iscrowd': [False, False]}, {'id': [355114, 362386, 362831, 364422, 364458, 394135, 1337463, 1388228, 1798342, 2053638], 'area': [419.7254999999999, 199.56624999999977, 172.36909999999997, 113.86720000000075, 46.00859999999981, 32148.63650000001, 217.95479999999972, 1475.563499999999, 180.82650000000004, 2502.7873999999997], 'bbox': [[492.1400146484375, 228.1999969482422, 30.1299991607666, 18.450000762939453], [524.7100219726562, 226.30999755859375, 19.549999237060547, 13.779999732971191], [555.1300048828125, 217.67999267578125, 28.979999542236328, 8.859999656677246], [533.030029296875, 222.4199981689453, 19.059999465942383, 15.619999885559082], [583.9500122070312, 221.47999572753906, 9.90999984741211, 7.269999980926514], [138.1999969482422, 149.25999450683594, 276.4100036621094, 179.66000366210938], [511.4100036621094, 227.16000366210938, 19.329999923706055, 19.68000030517578], [553.469970703125, 290.95001220703125, 37.27000045776367, 75.88999938964844], [546.5599975585938, 212.9600067138672, 26.389999389648438, 12.239999771118164], [394.5, 221.6999969482422, 71.77999877929688, 52.599998474121094]], 'label': [3, 3, 3, 3, 3, 8, 3, 11, 8, 8], 'iscrowd': [False, False, False, False, False, False, False, False, False, False]}, {'id': [82194, 1129850], 'area': [719.4571500000005, 17206.651100000006], 'bbox': [[134.5399932861328, 348.3699951171875, 22.700000762939453, 48.91999816894531], [109.30000305175781, 384.0, 214.3000030517578, 100.66999816894531]], 'label': [44, 81], 'iscrowd': [False, False]}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 118287/118287 [00:10<00:00, 11101.77 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 12396.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "exeSpace = os.path.abspath(os.path.join(os.getcwd(), \"../..\")) + \"/\"\n",
    "targetPathBase = exeSpace + 'datasets/coco'\n",
    "PATH_TO_IMAGE_FOLDER = targetPathBase\n",
    "\n",
    "def create_full_path(example):\n",
    "    \"\"\"Create full path to image using `base_path` to COCO2017 folder.\"\"\"\n",
    "    example[\"image_path\"] = os.path.join(PATH_TO_IMAGE_FOLDER, example[\"file_name\"])\n",
    "    return example\n",
    "\n",
    "dataset = load_dataset(\"phiyodr/coco2017\")\n",
    "dataset = dataset.map(create_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_paths, N):\n",
    "    # Specify the GPU by index (e.g., use GPU 1)\n",
    "    gpu_index = 0  # Change this to the desired GPU index\n",
    "    device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load pretrained FLAVA model and move it to the correct device\n",
    "    model = flava_model(pretrained=True).to(device)\n",
    "    model.eval()\n",
    "    # Define the image transform using FLAVA's image preprocessing\n",
    "    image_transform = FLAVAImageTransform(is_train=False)\n",
    "    image_tensors = []\n",
    "    \n",
    "    # Process the first N images in the list\n",
    "    for image_path in image_paths[:N]:\n",
    "        # Open the image file\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB mode\n",
    "        # Apply the FLAVA image transform\n",
    "        image_tensor = image_transform(image)[\"image\"].unsqueeze(0)  # Add batch dimension\n",
    "        image_tensors.append(image_tensor)\n",
    "    \n",
    "    # Stack the image tensors into a batch\n",
    "    image_tensors = torch.cat(image_tensors, dim=0).to(device)  # Move to GPU if available\n",
    "    \n",
    "    # Encode the images using FLAVA's image encoder\n",
    "    with torch.no_grad():\n",
    "        _, image_embeddings = model.encode_image(image_tensors, projection=True)\n",
    "    \n",
    "    return image_embeddings\n",
    "import numpy as np\n",
    "def append_to_fvecs(file_path, vectors):\n",
    "        \"\"\" Appends the vectors to an .fvecs file. \"\"\"\n",
    "        with open(file_path, 'ab') as f:\n",
    "            for vec in vectors:\n",
    "                dim = np.array([vec.shape[0]], dtype=np.int32)  # First write the dimension\n",
    "                vec = vec.cpu().numpy().astype(np.float32)       # Convert to numpy float32\n",
    "                dim.tofile(f)                                    # Write dimension\n",
    "                vec.tofile(f)       \n",
    "# Micro-batched image encoder with flushing after every batch\n",
    "def encode_images_to_fvecs(image_paths, N, batch_size, output_file):\n",
    "    total_images = min(N, len(image_paths))  # Ensure we don't exceed available images\n",
    "     # Specify the GPU by index (e.g., use GPU 1)\n",
    "    gpu_index = 0  # Change this to the desired GPU index\n",
    "    device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load pretrained FLAVA model and move it to the correct device\n",
    "    model = flava_model(pretrained=True).to(device)\n",
    "    model.eval()\n",
    "    # Define the image transform using FLAVA's image preprocessing\n",
    "    image_transform = FLAVAImageTransform(is_train=False)\n",
    "    image_tensors = []\n",
    "    # Process images in micro-batches and flush after every batch\n",
    "    for i in range(0, total_images, batch_size):\n",
    "        # Get the paths for the current micro-batch\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        \n",
    "        image_tensors = []\n",
    "        for image_path in batch_paths:\n",
    "            # Open the image file\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB mode\n",
    "            # Apply the FLAVA image transform\n",
    "            image_tensor = image_transform(image)[\"image\"].unsqueeze(0)  # Add batch dimension\n",
    "            image_tensors.append(image_tensor)\n",
    "        \n",
    "        # Stack the image tensors into a batch\n",
    "        image_tensors = torch.cat(image_tensors, dim=0).to(device)  # Move to GPU if available\n",
    "        \n",
    "        # Encode the images using FLAVA's image encoder\n",
    "        with torch.no_grad():\n",
    "            _, image_embeddings = model.encode_image(image_tensors, projection=True)\n",
    "        \n",
    "        # Append the encoded embeddings to the .fvecs file\n",
    "        append_to_fvecs(output_file, image_embeddings)\n",
    "\n",
    "        print(f\"Processed batch {i // batch_size + 1}, flushed to {output_file}\")\n",
    "\n",
    "    print(f\"Finished encoding {total_images} images and saved to {output_file}\")\n",
    "# Micro-batched caption encoder with flushing after every batch\n",
    "def encode_captions(captions, N, batch_size, output_file):\n",
    "    total_captions = min(N, len(captions))  # Ensure we don't exceed available captions\n",
    "    # Load BERT tokenizer from Hugging Face for text tokenization\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "     # Specify the GPU by index (e.g., use GPU 1)\n",
    "    gpu_index = 0  # Change this to the desired GPU index\n",
    "    device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load pretrained FLAVA model and move it to the correct device\n",
    "    model = flava_model(pretrained=True).to(device)\n",
    "    model.eval()\n",
    "    # Process captions in micro-batches and flush after every batch\n",
    "    for i in range(0, total_captions, batch_size):\n",
    "        # Get the captions for the current micro-batch\n",
    "        batch_captions = captions[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize captions and convert to tensors\n",
    "        inputs = tokenizer(batch_captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        text_tensors = inputs['input_ids'].to(device)  # Move to GPU if available\n",
    "        \n",
    "        # Encode the texts using FLAVA's text encoder\n",
    "        with torch.no_grad():\n",
    "            _, text_embeddings = model.encode_text(text_tensors, projection=True)\n",
    "        \n",
    "        # Append the encoded embeddings to the .fvecs file\n",
    "        append_to_fvecs(output_file, text_embeddings)\n",
    "\n",
    "        print(f\"Processed batch {i // batch_size + 1}, flushed to {output_file}\")\n",
    "\n",
    "    print(f\"Finished encoding {total_captions} captions and saved to {output_file}\")\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# Function to read vectors from an *.fvecs file\n",
    "def read_fvecs(file_path):\n",
    "    vectors = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            # Read the dimension (first 4 bytes)\n",
    "            dim_bytes = f.read(4)\n",
    "            if not dim_bytes:\n",
    "                break  # End of file\n",
    "            dim = np.frombuffer(dim_bytes, dtype=np.int32)[0]\n",
    "            \n",
    "            # Read the vector based on the dimension\n",
    "            vec = np.frombuffer(f.read(4 * dim), dtype=np.float32)\n",
    "            vectors.append(vec)\n",
    "    return vectors\n",
    "\n",
    "# Function to append two *.fvecs files and save the result into a new file\n",
    "def append_fvecs(file1, file2, output_file):\n",
    "    # Read vectors from both fvecs files\n",
    "    vectors1 = read_fvecs(file1)\n",
    "    vectors2 = read_fvecs(file2)\n",
    "    \n",
    "    # Combine the vectors\n",
    "    combined_vectors = vectors1 + vectors2\n",
    "    \n",
    "    # Save the combined vectors to a new .fvecs file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        for vec in combined_vectors:\n",
    "            dim = np.array([vec.shape[0]], dtype=np.int32)  # Write dimension\n",
    "            dim.tofile(f)\n",
    "            vec.astype(np.float32).tofile(f)  # Write vector values\n",
    "\n",
    "    print(f\"Appended {len(vectors2)} vectors from {file2} to {file1}, saved to {output_file}\")\n",
    "\n",
    "# Function to shuffle and save combined vectors into a new *.fvecs file\n",
    "def shuffle_and_save_fvecs(file1, file2, output_file):\n",
    "    # Read vectors from both fvecs files\n",
    "    vectors1 = read_fvecs(file1)\n",
    "    vectors2 = read_fvecs(file2)\n",
    "    \n",
    "    # Combine the vectors from both files\n",
    "    combined_vectors = vectors1 + vectors2\n",
    "    \n",
    "    # Shuffle the combined vectors\n",
    "    random.shuffle(combined_vectors)\n",
    "    \n",
    "    # Save the shuffled vectors to a new .fvecs file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        for vec in combined_vectors:\n",
    "            dim = np.array([vec.shape[0]], dtype=np.int32)  # Write dimension\n",
    "            dim.tofile(f)\n",
    "            vec.astype(np.float32).tofile(f)  # Write vector values\n",
    "\n",
    "    print(f\"Shuffled {len(combined_vectors)} vectors from {file1} and {file2}, saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A man with a red helmet on a small moped on a dirt road. ', 'Man riding a motor bike on a dirt road on the countryside.', 'A man riding on the back of a motorcycle.', 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ', 'A man in a red shirt and a red hat is on a motorcycle on a hill side.']\n",
      "A woman wearing a net on her head cutting a cake. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianzhi/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianzhi/.local/lib/python3.10/site-packages/torchmultimodal/utils/common.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(local_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1, flushed to captions.fvecs\n",
      "Finished encoding 128 captions and saved to captions.fvecs\n",
      "Using device: cuda:0\n",
      "Processed batch 1, flushed to image.fvecs\n",
      "Finished encoding 128 images and saved to image.fvecs\n"
     ]
    }
   ],
   "source": [
    "print((dataset['train']['captions'][0]))\n",
    "captions = dataset['train']['captions']\n",
    "captionFirst =  [caption[0] for caption in captions[:len(captions)]]\n",
    "print(captionFirst[1])\n",
    "caps = encode_captions(captionFirst,128,128,'captions.fvecs')\n",
    "images = encode_images_to_fvecs(dataset['train']['image_path'],128,128,'image.fvecs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'image_id', 'ids', 'captions', 'image_path'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
