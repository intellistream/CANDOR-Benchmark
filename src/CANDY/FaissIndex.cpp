//
// Created by Isshin on 2024/1/30.
//
#include <CANDY/FaissIndex.h>
#include <faiss/IndexHNSW.h>
#include <faiss/IndexIVFPQ.h>
#include <faiss/IndexPQ.h>
#include <faiss/IndexNNDescent.h>
#include <faiss/IndexLSH.h>
#include <faiss/IndexNSG.h>
#include <faiss/IndexHNSWbd.h>
bool CANDY::FaissIndex::setConfig(INTELLI::ConfigMapPtr cfg) {
  AbstractIndex::setConfig(cfg);
  INTELLI_INFO("SETTING CONFIG FOR FaissIndex");
  std::string metricType = cfg->tryString("metricType", "L2", true);
  vecDim = cfg->tryI64("vecDim", 768, true);
  index_type = cfg->tryString("faissIndexTag", "flat", true);
  auto bytes = cfg->tryI64("encodeLen", 1, true);
  INTELLI_INFO("USING " + metricType + " AS METRIC");
  if (index_type == "flat") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE FLAT");
    index = new faiss::IndexFlat(vecDim, faissMetric);
  } else if (index_type == "HNSW") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE HNSWFlat");
    auto M = cfg->tryI64("maxConnection", 32, true);
    index = new faiss::IndexHNSWFlat(vecDim, M, faissMetric);
  } else if (index_type == "HNSWbd") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE HNSWFlat with breakdown enabled!");
    auto M = cfg->tryI64("maxConnection", 32, true);
    index = new faiss::IndexHNSWbdFlat(vecDim, M, faissMetric);
} else if (index_type == "PQ") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE PQ");
    // number of bits in PQ
    auto nbits = cfg->tryI64("encodeLenBits", bytes * 8, true);
    bool is_online = cfg->tryI64("isOnlinePQ", 0, true);
    if (is_online == 1) {
      INTELLI_INFO("INITIALIZE AS ONLINE PQ!");
    }
    // number of subquantizers in PQ
    auto M = cfg->tryI64("subQuantizers", 8, true);

    if (vecDim == 420 || vecDim == 100) {
      index = new faiss::IndexPQ(is_online, vecDim + 4, M, nbits, faissMetric);
    } else if (vecDim == 1369) {
      index = new faiss::IndexPQ(is_online, vecDim + 7, M, nbits, faissMetric);
    } else {
      index = new faiss::IndexPQ(is_online, vecDim, M, nbits, faissMetric);
    }
  } else if (index_type == "IVFPQ") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE IVFPQ");
    auto nlist = cfg->tryI64("nlist", 1000, true);
    auto M = cfg->tryI64("subQuantizers", 8, true);
    auto nbits = cfg->tryI64("encodeLenBits", bytes * 8, true);
    // Hard-coded for msong and glove
    if (vecDim == 420 || vecDim == 100) {
      faiss::IndexFlat *quantizer = new faiss::IndexFlat(vecDim + 4, faissMetric);
      index = new faiss::IndexIVFPQ(quantizer, vecDim + 4, nlist, M, nbits);
    } else if (vecDim == 1369) {
      faiss::IndexFlat *quantizer = new faiss::IndexFlat(vecDim + 7, faissMetric);
      index = new faiss::IndexIVFPQ(quantizer, vecDim + 7, nlist, M, nbits);
    } else {
      faiss::IndexFlat *quantizer = new faiss::IndexFlat(vecDim, faissMetric);
      index = new faiss::IndexIVFPQ(quantizer, vecDim, nlist, M, nbits);
    }
  } else if (index_type == "NNDescent") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE NNDescent");
    auto K = cfg->tryI64("K", 5, true);
    index = new faiss::IndexNNDescentFlat(vecDim, K);
  } else if (index_type == "LSH") {

    auto nbits = cfg->tryI64("encodeLenBits", bytes * 32, true);
    index = new faiss::IndexLSH(vecDim, nbits, faissMetric, true, true);
    //index = new faiss::IndexLSH(vecDim, nbits,true,true);
  } else if (index_type == "NSG") {
    INTELLI_INFO("ENCAPSULATED FAISS INDEX: USE NSG");
    auto R = cfg->tryI64("R", 32, true);
    index = new faiss::IndexNSGFlat(vecDim, R);
  } else {
    INTELLI_INFO("NOT AN ENCAPSULATED FAISS INDEX: USE FLAT AS DEFAULT");
    index = new faiss::IndexFlat(vecDim, faissMetric);
  }
  index->set_verbose(true);
  dbTensor = torch::zeros({0, vecDim});
  lastNNZ = -1;
  expandStep = 100;
  return true;
}

bool CANDY::FaissIndex::loadInitialTensor(torch::Tensor &t) {

  // normalize the tensor before insert
  if (metricType == "cossim") {
    t = t / (torch::sqrt(torch::sum(t * t, 1)).view({t.size(0), 1}));
  }

  auto n = t.size(0);
  float *new_data = t.contiguous().data_ptr<float>();
  isFaissTrained = true;
  if (index_type == "IVFPQ" || index_type == "PQ") {
    INTELLI_INFO("IMCOMPATIBLE DIMENSIONS: PADDING ZEROS FOR PQ and IVFPQ");
    if (vecDim == 100 || vecDim == 420) {

      auto t_temp = torch::concat({t, torch::zeros({n, 4})}, 1);
      //t_temp.slice(1,t_temp.size(1)-4,t_temp.size(1)) -= t_temp.slice(1,t_temp.size(1)-4, t_temp.size(1));
      t_temp = t_temp.nan_to_num(0.0);
      t_temp = t_temp.contiguous();
      float *new_data_padding = t_temp.data_ptr<float>();

      INTELLI_INFO("Start training");
      index->train(n, new_data_padding);

      INTELLI_INFO("FINISH TRAINING");
      index->add(n, new_data_padding);

      INTELLI_INFO("FINISH ADDING");

      return INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
    } else if (vecDim == 1369) {
      auto t_temp = torch::concat({t, torch::zeros({n, 7})}, 1);
      t_temp = t_temp.nan_to_num(0.0);
      float *new_data_padding = t_temp.data_ptr<float>();

      INTELLI_INFO("Start training");
      index->train(n, new_data_padding);

      INTELLI_INFO("FINISH TRAINING");
      index->add(n, new_data_padding);
      INTELLI_INFO("FINISH ADDING");
      return INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
    }
  }

  INTELLI_INFO("Start training");
  index->train(n, new_data);

  INTELLI_INFO("FINISH TRAINING");
  std::cout << "tiny wait" << std::endl;
  index->add(n, new_data);
  INTELLI_INFO("FINISH ADDING");
  if (index_type == "IVFPQ" || index_type == "PQ" || index_type == "LSH") {
    return INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
  } else {
    return true;
  }

}
bool CANDY::FaissIndex::insertTensor(torch::Tensor &t) {
  // normalize the tensor before insert
  if (metricType == "cossim") {
    t = t / (torch::sqrt(torch::sum(t * t, 1)).view({t.size(0), 1}));
  }
  if (!isFaissTrained) {
    loadInitialTensor(t);
    isFaissTrained = true;
    return false;
  }
  float *new_data = t.contiguous().data_ptr<float>();
  auto n = t.size(0);
  if (index_type == "IVFPQ" || index_type == "PQ") {
    if (vecDim == 100 || vecDim == 420) {
      auto t_temp = torch::zeros({n, vecDim + 4});
      t_temp.slice(1, 0, vecDim) = t;

      t_temp = t_temp.nan_to_num(0.0);
      float *new_data_padding = t_temp.contiguous().data_ptr<float>();
      index->add(n, new_data_padding);
      return INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
    } else if (vecDim == 1369) {
      auto t_temp = torch::zeros({n, vecDim + 7});
      t_temp.slice(1, 0, vecDim) = t;
      std::cout << "inserting" << std::endl;

      t_temp = t_temp.nan_to_num(0.0);
      float *new_data_padding = t_temp.contiguous().data_ptr<float>();
      index->add(n, new_data_padding);
      return INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
    }
  }
  if (index_type == "IVFPQ" || index_type == "PQ" || index_type == "LSH") {
    index->add(n, new_data);
    return INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
  } else {
    index->add(n, new_data);
    //should be unneeded
    INTELLI::IntelliTensorOP::appendRowsBufferMode(&dbTensor, &t, &lastNNZ, expandStep);
    return true;
  }
}

std::vector<faiss::idx_t> CANDY::FaissIndex::searchIndex(torch::Tensor q, int64_t k) {

  auto queryData = q.contiguous().data_ptr<float>();
  int64_t querySize = q.size(0);

  if (index_type == "IVFPQ" || index_type == "PQ") {
    INTELLI_INFO("IMCOMPATIBLE DIMENSIONS: PADDING ZEROS FOR PQ and IVFPQ");
    if (vecDim == 100 || vecDim == 420) {
      auto q_temp = torch::zeros({querySize, vecDim + 4});
      q_temp.slice(1, 0, vecDim) = q;
      q_temp = q_temp.nan_to_num(0.0);
      auto queryData_padding = q_temp.contiguous().data_ptr<float>();
      std::vector<faiss::idx_t> ru(k * querySize);
      std::vector<float> distance(k * querySize);
      index->search(querySize, queryData_padding, k, distance.data(), ru.data());
      return ru;
    } else if (vecDim == 1369) {
      auto q_temp = torch::zeros({querySize, vecDim + 7});
      q_temp.slice(1, 0, vecDim) = q;
      std::cout << "tiny wait" << std::endl;
      q_temp = q_temp.nan_to_num(0.0);
      auto queryData_padding = q_temp.contiguous().data_ptr<float>();
      std::vector<faiss::idx_t> ru(k * querySize);
      std::vector<float> distance(k * querySize);
      index->search(querySize, queryData_padding, k, distance.data(), ru.data());
      return ru;
    }
  }

  std::vector<faiss::idx_t> ru(k * querySize);
  std::vector<float> distance(k * querySize);
  index->search(querySize, queryData, k, distance.data(), ru.data());
  return ru;
}

std::vector<torch::Tensor> CANDY::FaissIndex::searchTensor(torch::Tensor &q, int64_t k) {
  auto idx = searchIndex(q, k);
  return getTensorByIndex(idx, k);
}

std::vector<torch::Tensor> CANDY::FaissIndex::getTensorByIndex(std::vector<faiss::idx_t> &idx, int64_t k) {
  int64_t size = idx.size() / k;
  std::vector<torch::Tensor> ru(size);
  for (int64_t i = 0; i < size; i++) {
    ru[i] = torch::zeros({k, vecDim});
    for (int64_t j = 0; j < k; j++) {
      int64_t tempIdx = idx[i * k + j];
      float tempSlice[vecDim];
//            if(index_type=="FaissIVFPQ" || index_type == "FaissPQ"){
//                if(vecDim=100 || vecDim == 420){
//
//                } else if (vecDim==1369){
//
//                }
//            }
      if (index_type == "IVFPQ" || index_type == "PQ" || index_type == "LSH") {
        if (tempIdx >= 0) {
          ru[i].slice(0, j, j + 1) = dbTensor.slice(0, tempIdx, tempIdx + 1);
        };
      } else {
        index->reconstruct(tempIdx, tempSlice);
        auto tempTensor = torch::from_blob(tempSlice, {1, vecDim});
        if (tempIdx >= 0) { ru[i].slice(0, j, j + 1) = tempTensor; };
      }

    }
  }
  return ru;
}
